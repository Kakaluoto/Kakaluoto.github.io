<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>bert finetune | Kakaluoto's Blog</title><meta name="author" content="HY"><meta name="copyright" content="HY"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一. 回顾TransformerTransformer就是结合了自注意力机制的encoder-decoder网络。  图的左边是Encoder右边是Decoder 二. BERT是什么有什么用1.BERT的优点​       在我们熟知的图像处理方向，模型微调的技术已经十分的成熟，我们CNN在学习过程中可以提取到一些深层特征和浅层特征，浅层的CNN网络往往学习到一些边缘，形状的很低级的特征，而深层">
<meta property="og:type" content="article">
<meta property="og:title" content="bert finetune">
<meta property="og:url" content="http://example.com/2021/11/16/bert_finetune/index.html">
<meta property="og:site_name" content="Kakaluoto&#39;s Blog">
<meta property="og:description" content="一. 回顾TransformerTransformer就是结合了自注意力机制的encoder-decoder网络。  图的左边是Encoder右边是Decoder 二. BERT是什么有什么用1.BERT的优点​       在我们熟知的图像处理方向，模型微调的技术已经十分的成熟，我们CNN在学习过程中可以提取到一些深层特征和浅层特征，浅层的CNN网络往往学习到一些边缘，形状的很低级的特征，而深层">
<meta property="og:locale">
<meta property="og:image" content="https://z3.ax1x.com/2021/11/16/Ihe4rn.jpg">
<meta property="article:published_time" content="2021-11-16T14:56:25.719Z">
<meta property="article:modified_time" content="2021-11-16T15:00:04.301Z">
<meta property="article:author" content="HY">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://z3.ax1x.com/2021/11/16/Ihe4rn.jpg"><link rel="shortcut icon" href="https://z3.ax1x.com/2021/10/27/5b8HIA.png"><link rel="canonical" href="http://example.com/2021/11/16/bert_finetune/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'bert finetune',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-11-16 23:00:04'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mouse.css"><link rel="stylesheet" href="/css/footer.css"><link rel="stylesheet" href="/css/szgotop.css" /><script src="/js/jquery.min.js"></script><div id="aplayer"></div><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js" async></script><div class="back-to-top cd-top faa-float animated cd-is-visible" style="top:-900px;"></div><link rel="stylesheet" href="/css/szgotop.css"><script src="/js/szgotop.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-butterfly-clock/lib/clock.min.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://z3.ax1x.com/2021/10/27/57Qjns.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E6%A0%87%E7%AD%BE"><i class="fa-fw /tags/"></i><span> 0</span></a></li><li><a class="site-page child" href="/%E5%88%86%E7%B1%BB"><i class="fa-fw /categories/"></i><span> 1</span></a></li><li><a class="site-page child" href="/%E6%97%B6%E9%97%B4%E8%BD%B4"><i class="fa-fw /archives/"></i><span> 2</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-link"></i><span> 链接</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5"><i class="fa-fw /link/"></i><span> 0</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-list"></i><span> 菜单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E5%85%B3%E4%BA%8E"><i class="fa-fw /about/"></i><span> 0</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://z3.ax1x.com/2021/11/16/Ihe4rn.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Kakaluoto's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E6%A0%87%E7%AD%BE"><i class="fa-fw /tags/"></i><span> 0</span></a></li><li><a class="site-page child" href="/%E5%88%86%E7%B1%BB"><i class="fa-fw /categories/"></i><span> 1</span></a></li><li><a class="site-page child" href="/%E6%97%B6%E9%97%B4%E8%BD%B4"><i class="fa-fw /archives/"></i><span> 2</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-link"></i><span> 链接</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5"><i class="fa-fw /link/"></i><span> 0</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-list"></i><span> 菜单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/%E5%85%B3%E4%BA%8E"><i class="fa-fw /about/"></i><span> 0</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">bert finetune</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-11-16T14:56:25.719Z" title="Created 2021-11-16 22:56:25">2021-11-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-11-16T15:00:04.301Z" title="Updated 2021-11-16 23:00:04">2021-11-16</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="bert finetune"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="一-回顾Transformer"><a href="#一-回顾Transformer" class="headerlink" title="一. 回顾Transformer"></a>一. 回顾Transformer</h2><p>Transformer就是结合了自注意力机制的encoder-decoder网络。</p>
<p><img src="https://z3.ax1x.com/2021/11/16/IhZm6A.jpg" style="zoom:50%;" /></p>
<p>图的左边是Encoder右边是Decoder</p>
<h2 id="二-BERT是什么有什么用"><a href="#二-BERT是什么有什么用" class="headerlink" title="二. BERT是什么有什么用"></a>二. BERT是什么有什么用</h2><h3 id="1-BERT的优点"><a href="#1-BERT的优点" class="headerlink" title="1.BERT的优点"></a>1.BERT的优点</h3><p>​       在我们熟知的图像处理方向，模型微调的技术已经十分的成熟，我们CNN在学习过程中可以提取到一些深层特征和浅层特征，浅层的CNN网络往往学习到一些边缘，形状的很低级的特征，而深层网络可以学习到更加高级的特征，这些特征在许多图像处理问题中是通用的，所以可以载入预先训练好的模型权重，根据具体问题修改输出层进行再训练就可以取得比较好的效果。</p>
<p>​        在NLP领域的模型预训练和微调，就是采用的BERT,我们可以用特定的文本处理任务训练BERT,再将训练好的BERT模型进行魔改以适应不同的任务。</p>
<p>​        BERT有如下的特点：</p>
<ul>
<li><p>采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。</p>
</li>
<li><p>预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得很好的表现。在这过程中并不需要对BERT进行任务特定的结构修改。</p>
</li>
</ul>
<h3 id="2-BERT与Transformer的联系"><a href="#2-BERT与Transformer的联系" class="headerlink" title="2. BERT与Transformer的联系"></a>2. BERT与Transformer的联系</h3><p>我们可能都知道BERT只采用了Transformer的Encoder部分，但实际上这样的说法很容易让人产生误解，让人以为BERT只是Transformer的缩小版而已，实际上BERT模型比是Transformer大几个数量级。</p>
<p>下图更加直观地展示了BERT与Transformer之间的关系。</p>
<p><img src="https://z3.ax1x.com/2021/11/16/IhZNXn.jpg" style="zoom:80%;" /><br>BERT将Transformer的编码器部分进行叠加。</p>
<p>论文中的BERT提供了简单和复杂两个模型，对应的超参数分别如下：</p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BBERT%7D_%7B%5Cmathbf%7BBASE%7D%7D" alt="[公式]"> : L=12，H=768，A=12，参数总量110M；</li>
<li><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BBERT%7D_%7B%5Cmathbf%7BLARGE%7D%7D" alt="[公式]"> : L=24，H=1024，A=16，参数总量340M；</li>
</ul>
<p>在上面的超参数中，L表示网络的层数（即Transformer blocks的数量），A表示Multi-Head Attention中self-Attention的数量，filter的尺寸是4H。</p>
<h2 id="三-BERT预训练"><a href="#三-BERT预训练" class="headerlink" title="三. BERT预训练"></a>三. BERT预训练</h2><h3 id="1-关于自监督学习Self-supervised-learning"><a href="#1-关于自监督学习Self-supervised-learning" class="headerlink" title="1. 关于自监督学习Self-supervised-learning"></a>1. 关于自监督学习Self-supervised-learning</h3><p><img src="https://z3.ax1x.com/2021/11/16/IhZyp4.png" style="zoom:67%;" /><br>自监督学习与监督学习的区别在于标签并不是特意标注的，而是通过训练数据集自己产生的，而BERT可以看作是一种自监督学习模型，即BERT在进行与训练的时候所用的标签其实就是来自与文本数据自身。</p>
<p>具体是如何做到的呢？主要是通过两个预训练任务的设计来实现的，一个是MLM(Masked Language Model),另一个是NSP(Next Sentence Prediction),这个之后再解释。</p>
<h3 id="2-BERT输入表示"><a href="#2-BERT输入表示" class="headerlink" title="2. BERT输入表示"></a>2. BERT输入表示</h3><p><img src="https://z3.ax1x.com/2021/11/16/IhZbjA.jpg" style="zoom: 80%;" /></p>
<p>BERT的输入为每一个token对应的表征<em>（图中的粉红色块就是token，黄色块就是token对应的表征）</em>，并且单词字典是采用WordPiece算法来进行构建的。</p>
<p><img src="https://z3.ax1x.com/2021/11/16/Ihep9g.jpg"  /></p>
<p>每一个token对应的表征组成一个编码向量（长度是512），该编码向量是3个嵌入特征的单位和，这三个词嵌入特征是：</p>
<ol>
<li><p>WordPiece 嵌入：</p>
<p>WordPiece是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。例如将‘playing’被拆分成了‘play’和‘ing’；</p>
</li>
<li><p>位置嵌入（Position Embedding）：</p>
<p>位置嵌入是指将单词的位置信息编码成特征向量，位置嵌入是向模型中引入单词位置关系的至关重要的一环。</p>
<p>位置嵌入其实就是一种代替位置编码的方法，当初讲自注意力机制的时候，使用的是不用带参数的位置编码，并且采用的是三角函数，为的是让输入数据带有位置信息，而BERT延续了这一思想，只不过将参数全部改成了可以学习的参数。</p>
</li>
<li><p>分割嵌入（Segment Embedding）：用于区分两个句子，例如B是否是A的下文（对话场景，问答场景等）。对于句子对，第一个句子的特征值是0，第二个句子的特征值是1。</p>
</li>
</ol>
<p>两个特殊符号<code>[CLS]</code>和<code>[SEP]</code>，其中<code>[CLS]</code>表示该特征用于分类模型，对非分类模型，该符合可以省去。<code>[SEP]</code>表示分句符号，用于断开输入语料中的两个句子。</p>
<h3 id="3-BERT预训练任务"><a href="#3-BERT预训练任务" class="headerlink" title="3. BERT预训练任务"></a>3. BERT预训练任务</h3><h4 id="3-1-Masked-Language-Model（MLM）"><a href="#3-1-Masked-Language-Model（MLM）" class="headerlink" title="3.1 Masked Language Model（MLM）"></a>3.1 Masked Language Model（MLM）</h4><p>​        所谓MLM是指在训练的时候随即从输入预料上mask掉一些单词，然后通过的上下文预测该单词，该任务非常像完形填空。</p>
<p>​        mask的好处，即预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（ 10% 概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。</p>
<p><img src="https://z3.ax1x.com/2021/11/16/IheVEV.png" style="zoom: 50%;" /></p>
<p>这个mask可以是特殊符号‘<mask>’也可以是随机选取一些乱七八糟的符号，又或者是原来正确的符号。</p>
<p>在BERT的实验中，15%的WordPiece Token会被随机Mask掉。在训练模型时，一个句子会被多次喂到模型中用于参数学习，但是Google并没有在每次都mask掉这些单词，而是在确定要Mask掉的单词之后，80%的时候会直接替换为[Mask]，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token。</p>
<ul>
<li>80%：<code>my dog is hairy -&gt; my dog is [mask]</code></li>
<li>10%：<code>my dog is hairy -&gt; my dog is apple</code></li>
<li>10%：<code>my dog is hairy -&gt; my dog is hairy</code></li>
</ul>
<p>​        这么做的原因是如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词。加入随机Token的原因是因为Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘，但是实际fine-tune的时候并没有[mask]这个符号。至于随机单词带来的负面影响，因为一个单词被随机替换掉的概率只有15%*10% =1.5%，这个负面影响其实是可以忽略不计的。</p>
<h4 id="3-2-Next-Sentence-Prediction（NSP）"><a href="#3-2-Next-Sentence-Prediction（NSP）" class="headerlink" title="3.2 Next Sentence Prediction（NSP）"></a>3.2 Next Sentence Prediction（NSP）</h4><p>​        为了帮助 理解两个⽂本序列之间的关系，BERT在预训练中考虑了⼀个⼆元分类任务——下⼀句预测。在为预训练⽣成 句⼦对时，有⼀半的时间它们确实是标签为“真”的连续句⼦；在另⼀半的时间⾥，第⼆个句⼦是从语料库 中随机抽取的，标记为“假”。</p>
<ul>
<li>预测一个句子对中的两个句子是否相邻</li>
<li>训练样本中：<ul>
<li>50%概率选择相邻句子对:<cls>this movie is great <seq>i like it<sep></li>
<li>50%概率选择随机句子对:<cls>this movie is great <seq>hello world<sep></li>
<li>将<cls>对应的输出放到一个全连接层来预测</li>
</ul>
</li>
</ul>
<p><img src="https://z3.ax1x.com/2021/11/16/Iheu34.png" style="zoom: 50%;" /></p>
<p>​       关于NSP是否有用是存在争议的，(参见RoBERTa这篇论文)。因为选定一个句子作为前一个句子，大部分情况下随机选到的句子都不是后一个句子，也就是BERT是很容易判断出来两个句子是否相邻的。对此也有人提出了改进方法SOP，SOP更加侧重于判断两个句子的前后关系，而不是两个句子是否相邻。</p>
<h4 id="Tips-BERT预训练是多任务模型的预训练，因此NSP和MLM两个任务的训练是同时进行的。"><a href="#Tips-BERT预训练是多任务模型的预训练，因此NSP和MLM两个任务的训练是同时进行的。" class="headerlink" title="Tips: BERT预训练是多任务模型的预训练，因此NSP和MLM两个任务的训练是同时进行的。"></a>Tips: BERT预训练是多任务模型的预训练，因此NSP和MLM两个任务的训练是同时进行的。</h4><h2 id="三-BERT微调fine-tune"><a href="#三-BERT微调fine-tune" class="headerlink" title="三. BERT微调fine-tune"></a>三. BERT微调fine-tune</h2><h3 id="1-自然语言推理-Natural-Language-Inference"><a href="#1-自然语言推理-Natural-Language-Inference" class="headerlink" title="1. 自然语言推理(Natural Language Inference)"></a>1. 自然语言推理(Natural Language Inference)</h3><p>自然语言推理研究是否有假设可以从前提推断出来，其中两者都是文本序列。换句话说，自然语言推理决定了一对文本序列之间的逻辑关系。这种关系通常分为三种类型：</p>
<ul>
<li><em>蕴含（entailment）</em>：假设可以从前提中推断出来。</li>
<li><em>矛盾（contradiction）</em>：可以从前提中推断假设的否定。</li>
<li><em>中立（neutral）</em>：所有其他情况。</li>
</ul>
<p>自然语言推理也被称为识别文本蕴含任务。例如，下面的词元对将被标记为<em>蕴含（entailment）</em>，因为假设中的 “显示亲情” 可以从前提中的 “相互拥抱” 推断出来。</p>
<blockquote>
<p>前提：两个女人互相拥抱。</p>
<p>假设：两个女人表现出亲情。</p>
</blockquote>
<p>以下是 <em>矛盾</em> 的例子，因为 “运行编码示例” 表示 “没有睡觉” 而不是 “睡觉”。</p>
<blockquote>
<p>前提：一个男人正在运行从 “潜入深度学习” 中的编码示例。</p>
<p>假设：那个男人在睡觉。</p>
</blockquote>
<p>第三个例子显示了 <em> 中立性 </em> 关系，因为 “正在为我们表演” 这一事实既不能推断 “著名的” 也不是 “不出名”。</p>
<blockquote>
<p>前提：音乐家们正在为我们表演。</p>
<p>假设：音乐家很有名。</p>
</blockquote>
<h3 id="2-NLI微调模型"><a href="#2-NLI微调模型" class="headerlink" title="2. NLI微调模型"></a>2. NLI微调模型</h3><p>​        BERT在预训练好之后就可以通过更改输出层来完成不同的任务，本节所讲的fine-tune应用是自然语言推理Natural Language Inference</p>
<p><img src="https://z3.ax1x.com/2021/11/16/IheluR.png" style="zoom: 80%;" /></p>
<p>本次微调所用的输出层很简单，直接在BERT输出层添加一个多层感知机。</p>
<h3 id="3-SNLI数据集"><a href="#3-SNLI数据集" class="headerlink" title="3. SNLI数据集"></a>3. SNLI数据集</h3><p>To Be Continued !</p>
<p>。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">HY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2021/11/16/bert_finetune/">http://example.com/2021/11/16/bert_finetune/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://z3.ax1x.com/2021/11/16/Ihe4rn.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/11/14/%E8%8B%B1%E6%96%87%E8%AE%BA%E6%96%87%E5%A5%97%E8%AF%9D/"><img class="next-cover" src="https://z3.ax1x.com/2021/11/14/I6ZhvT.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">英文论文套话</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://z3.ax1x.com/2021/10/27/57Qjns.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HY</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">4</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Kakaluoto"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content"><img src="https://z3.ax1x.com/2021/10/27/5bZeqP.gif"></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-%E5%9B%9E%E9%A1%BETransformer"><span class="toc-number">1.</span> <span class="toc-text">一. 回顾Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-BERT%E6%98%AF%E4%BB%80%E4%B9%88%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8"><span class="toc-number">2.</span> <span class="toc-text">二. BERT是什么有什么用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-BERT%E7%9A%84%E4%BC%98%E7%82%B9"><span class="toc-number">2.1.</span> <span class="toc-text">1.BERT的优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-BERT%E4%B8%8ETransformer%E7%9A%84%E8%81%94%E7%B3%BB"><span class="toc-number">2.2.</span> <span class="toc-text">2. BERT与Transformer的联系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89-BERT%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">3.</span> <span class="toc-text">三. BERT预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%85%B3%E4%BA%8E%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0Self-supervised-learning"><span class="toc-number">3.1.</span> <span class="toc-text">1. 关于自监督学习Self-supervised-learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-BERT%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA"><span class="toc-number">3.2.</span> <span class="toc-text">2. BERT输入表示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1"><span class="toc-number">3.3.</span> <span class="toc-text">3. BERT预训练任务</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-Masked-Language-Model%EF%BC%88MLM%EF%BC%89"><span class="toc-number">3.3.1.</span> <span class="toc-text">3.1 Masked Language Model（MLM）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-Next-Sentence-Prediction%EF%BC%88NSP%EF%BC%89"><span class="toc-number">3.3.2.</span> <span class="toc-text">3.2 Next Sentence Prediction（NSP）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Tips-BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%98%AF%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%8C%E5%9B%A0%E6%AD%A4NSP%E5%92%8CMLM%E4%B8%A4%E4%B8%AA%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AE%AD%E7%BB%83%E6%98%AF%E5%90%8C%E6%97%B6%E8%BF%9B%E8%A1%8C%E7%9A%84%E3%80%82"><span class="toc-number">3.3.3.</span> <span class="toc-text">Tips: BERT预训练是多任务模型的预训练，因此NSP和MLM两个任务的训练是同时进行的。</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89-BERT%E5%BE%AE%E8%B0%83fine-tune"><span class="toc-number">4.</span> <span class="toc-text">三. BERT微调fine-tune</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E6%8E%A8%E7%90%86-Natural-Language-Inference"><span class="toc-number">4.1.</span> <span class="toc-text">1. 自然语言推理(Natural Language Inference)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-NLI%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.2.</span> <span class="toc-text">2. NLI微调模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-SNLI%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.3.</span> <span class="toc-text">3. SNLI数据集</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/11/16/bert_finetune/" title="bert finetune"><img src="https://z3.ax1x.com/2021/11/16/Ihe4rn.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="bert finetune"/></a><div class="content"><a class="title" href="/2021/11/16/bert_finetune/" title="bert finetune">bert finetune</a><time datetime="2021-11-16T14:56:25.719Z" title="Created 2021-11-16 22:56:25">2021-11-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/11/14/%E8%8B%B1%E6%96%87%E8%AE%BA%E6%96%87%E5%A5%97%E8%AF%9D/" title="英文论文套话"><img src="https://z3.ax1x.com/2021/11/14/I6ZhvT.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="英文论文套话"/></a><div class="content"><a class="title" href="/2021/11/14/%E8%8B%B1%E6%96%87%E8%AE%BA%E6%96%87%E5%A5%97%E8%AF%9D/" title="英文论文套话">英文论文套话</a><time datetime="2021-11-14T01:11:19.063Z" title="Created 2021-11-14 09:11:19">2021-11-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/14/self_attention/" title="自注意力"><img src="https://z3.ax1x.com/2021/10/27/5HSkA1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="自注意力"/></a><div class="content"><a class="title" href="/2021/10/14/self_attention/" title="自注意力">自注意力</a><time datetime="2021-10-13T17:20:47.000Z" title="Created 2021-10-14 01:20:47">2021-10-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/14/hello-world/" title="Hello World"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/2021/10/14/hello-world/" title="Hello World">Hello World</a><time datetime="2021-10-13T17:18:48.660Z" title="Created 2021-10-14 01:18:48">2021-10-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By HY</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script src="/js/jquery.min.js"></script><script src="/js/szgotop.js"></script><div class="back-to-top cd-top faa-float animated cd-is-visible" style="top:-900px;"></div><div class="aplayer no-destroy" data-id="1461445180" data-server="netease" data-type="song" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="true" muted></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_clock_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_injector_config();
  }
  </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax src="https://cdn.jsdelivr.net/npm/hexo-butterfly-clock/lib/clock.min.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/hibiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>