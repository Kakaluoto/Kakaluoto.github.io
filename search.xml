<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Bert微调</title>
      <link href="/2021/11/16/bert_finetune/"/>
      <url>/2021/11/16/bert_finetune/</url>
      
        <content type="html"><![CDATA[<h3 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h3><ul><li><p><a href="https://zhuanlan.zhihu.com/p/98855346">什么是BERT？ - 知乎 (zhihu.com)</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/48612853">词向量之BERT - 知乎 (zhihu.com)</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/103226488">BERT 详解 - 知乎 (zhihu.com)</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/48508221">详解Transformer （Attention Is All You Need） - 知乎 (zhihu.com)</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/406786658">从Transformer到Bert - 知乎 (zhihu.com)</a></p></li><li><p><a href="https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html">14.10. 预训练BERT — 动手学深度学习 2.0.0-alpha2 documentation (d2l.ai)</a></p></li><li><p><a href="https://www.bilibili.com/video/av246993280?p=49">(强推)李宏毅2021春机器学习课程_哔哩哔哩_bilibili</a></p></li><li><p><a href="https://www.bilibili.com/video/av847491605">70 BERT微调【动手学深度学习v2】_哔哩哔哩_bilibili</a></p></li><li><p><a href="https://www.jianshu.com/p/2daf69f8408f">Bert细节整理 - 简书 (jianshu.com)</a></p></li></ul><h3 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h3><ul><li><a href="http://www.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html">http://www.d2l.ai/chapter_natural-language-processing-applications/natural-language-inference-bert.html</a></li></ul><h2 id="一-回顾Transformer"><a href="#一-回顾Transformer" class="headerlink" title="一. 回顾Transformer"></a>一. 回顾Transformer</h2><p>Transformer就是结合了自注意力机制的encoder-decoder网络。</p><p><img src="https://z3.ax1x.com/2021/11/16/IhZm6A.jpg" style="zoom:50%;" /></p><p>图的左边是Encoder右边是Decoder</p><h2 id="二-BERT是什么有什么用"><a href="#二-BERT是什么有什么用" class="headerlink" title="二. BERT是什么有什么用"></a>二. BERT是什么有什么用</h2><h3 id="1-BERT的优点"><a href="#1-BERT的优点" class="headerlink" title="1.BERT的优点"></a>1.BERT的优点</h3><p>​       在我们熟知的图像处理方向，模型微调的技术已经十分的成熟，我们CNN在学习过程中可以提取到一些深层特征和浅层特征，浅层的CNN网络往往学习到一些边缘，形状的很低级的特征，而深层网络可以学习到更加高级的特征，这些特征在许多图像处理问题中是通用的，所以可以载入预先训练好的模型权重，根据具体问题修改输出层进行再训练就可以取得比较好的效果。</p><p>​        在NLP领域的模型预训练和微调，就是采用的BERT,我们可以用特定的文本处理任务训练BERT,再将训练好的BERT模型进行魔改以适应不同的任务。</p><p>​        BERT有如下的特点：</p><ul><li><p>采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。</p></li><li><p>预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得很好的表现。在这过程中并不需要对BERT进行任务特定的结构修改。</p></li></ul><h3 id="2-BERT与Transformer的联系"><a href="#2-BERT与Transformer的联系" class="headerlink" title="2. BERT与Transformer的联系"></a>2. BERT与Transformer的联系</h3><p>我们可能都知道BERT只采用了Transformer的Encoder部分，但实际上这样的说法很容易让人产生误解，让人以为BERT只是Transformer的缩小版而已，实际上BERT模型比是Transformer大几个数量级。</p><p>下图更加直观地展示了BERT与Transformer之间的关系。</p><p><img src="https://z3.ax1x.com/2021/11/16/IhZNXn.jpg" style="zoom:80%;" /><br>BERT将Transformer的编码器部分进行叠加。</p><p>论文中的BERT提供了简单和复杂两个模型，对应的超参数分别如下：</p><ul><li><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BBERT%7D_%7B%5Cmathbf%7BBASE%7D%7D" alt="[公式]"> : L=12，H=768，A=12，参数总量110M；</li><li><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BBERT%7D_%7B%5Cmathbf%7BLARGE%7D%7D" alt="[公式]"> : L=24，H=1024，A=16，参数总量340M；</li></ul><p>在上面的超参数中，L表示网络的层数（即Transformer blocks的数量），A表示Multi-Head Attention中self-Attention的数量，filter的尺寸是4H。</p><h2 id="三-BERT预训练"><a href="#三-BERT预训练" class="headerlink" title="三. BERT预训练"></a>三. BERT预训练</h2><h3 id="1-关于自监督学习Self-supervised-learning"><a href="#1-关于自监督学习Self-supervised-learning" class="headerlink" title="1. 关于自监督学习Self-supervised-learning"></a>1. 关于自监督学习Self-supervised-learning</h3><p><img src="https://z3.ax1x.com/2021/11/16/IhZyp4.png" style="zoom:67%;" /><br>自监督学习与监督学习的区别在于标签并不是特意标注的，而是通过训练数据集自己产生的，而BERT可以看作是一种自监督学习模型，即BERT在进行与训练的时候所用的标签其实就是来自与文本数据自身。</p><p>具体是如何做到的呢？主要是通过两个预训练任务的设计来实现的，一个是MLM(Masked Language Model),另一个是NSP(Next Sentence Prediction),这个之后再解释。</p><h3 id="2-BERT输入表示"><a href="#2-BERT输入表示" class="headerlink" title="2. BERT输入表示"></a>2. BERT输入表示</h3><p><img src="https://z3.ax1x.com/2021/11/16/IhZbjA.jpg" style="zoom: 80%;" /></p><p>BERT的输入为每一个token对应的表征<em>（图中的粉红色块就是token，黄色块就是token对应的表征）</em>，并且单词字典是采用WordPiece算法来进行构建的。</p><p><img src="https://z3.ax1x.com/2021/11/16/Ihep9g.jpg"  /></p><p>每一个token对应的表征组成一个编码向量（长度是512），该编码向量是3个嵌入特征的单位和，这三个词嵌入特征是：</p><ol><li><p>WordPiece 嵌入：</p><p>WordPiece是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。例如将‘playing’被拆分成了‘play’和‘ing’；</p></li><li><p>位置嵌入（Position Embedding）：</p><p>位置嵌入是指将单词的位置信息编码成特征向量，位置嵌入是向模型中引入单词位置关系的至关重要的一环。</p><p>位置嵌入其实就是一种代替位置编码的方法，当初讲自注意力机制的时候，使用的是不用带参数的位置编码，并且采用的是三角函数，为的是让输入数据带有位置信息，而BERT延续了这一思想，只不过将参数全部改成了可以学习的参数。</p></li><li><p>分割嵌入（Segment Embedding）：用于区分两个句子，例如B是否是A的下文（对话场景，问答场景等）。对于句子对，第一个句子的特征值是0，第二个句子的特征值是1。</p></li></ol><p>两个特殊符号<code>[CLS]</code>和<code>[SEP]</code>，其中<code>[CLS]</code>表示该特征用于分类模型，对非分类模型，该符合可以省去。<code>[SEP]</code>表示分句符号，用于断开输入语料中的两个句子。</p><h3 id="3-BERT预训练任务"><a href="#3-BERT预训练任务" class="headerlink" title="3. BERT预训练任务"></a>3. BERT预训练任务</h3><h4 id="3-1-Masked-Language-Model（MLM）"><a href="#3-1-Masked-Language-Model（MLM）" class="headerlink" title="3.1 Masked Language Model（MLM）"></a>3.1 Masked Language Model（MLM）</h4><p>​        所谓MLM是指在训练的时候随即从输入预料上mask掉一些单词，然后通过的上下文预测该单词，该任务非常像完形填空。</p><p>​        mask的好处，即预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（ 10% 概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。</p><p><img src="https://z3.ax1x.com/2021/11/16/IheVEV.png" style="zoom: 50%;" /></p><p>这个mask可以是特殊符号‘<mask>’也可以是随机选取一些乱七八糟的符号，又或者是原来正确的符号。</p><p>在BERT的实验中，15%的WordPiece Token会被随机Mask掉。在训练模型时，一个句子会被多次喂到模型中用于参数学习，但是Google并没有在每次都mask掉这些单词，而是在确定要Mask掉的单词之后，80%的时候会直接替换为[Mask]，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token。</p><ul><li>80%：<code>my dog is hairy -&gt; my dog is [mask]</code></li><li>10%：<code>my dog is hairy -&gt; my dog is apple</code></li><li>10%：<code>my dog is hairy -&gt; my dog is hairy</code></li></ul><p>​        这么做的原因是如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词。加入随机Token的原因是因为Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘，但是实际fine-tune的时候并没有[mask]这个符号。至于随机单词带来的负面影响，因为一个单词被随机替换掉的概率只有15%*10% =1.5%，这个负面影响其实是可以忽略不计的。</p><h4 id="3-2-Next-Sentence-Prediction（NSP）"><a href="#3-2-Next-Sentence-Prediction（NSP）" class="headerlink" title="3.2 Next Sentence Prediction（NSP）"></a>3.2 Next Sentence Prediction（NSP）</h4><p>​        为了帮助 理解两个⽂本序列之间的关系，BERT在预训练中考虑了⼀个⼆元分类任务——下⼀句预测。在为预训练⽣成 句⼦对时，有⼀半的时间它们确实是标签为“真”的连续句⼦；在另⼀半的时间⾥，第⼆个句⼦是从语料库 中随机抽取的，标记为“假”。</p><ul><li>预测一个句子对中的两个句子是否相邻</li><li>训练样本中：<ul><li>50%概率选择相邻句子对:<cls>this movie is great <seq>i like it<sep></li><li>50%概率选择随机句子对:<cls>this movie is great <seq>hello world<sep></li><li>将<cls>对应的输出放到一个全连接层来预测</li></ul></li></ul><p><img src="https://z3.ax1x.com/2021/11/16/Iheu34.png" style="zoom: 50%;" /></p><p>​       关于NSP是否有用是存在争议的，(参见RoBERTa这篇论文)。因为选定一个句子作为前一个句子，大部分情况下随机选到的句子都不是后一个句子，也就是BERT是很容易判断出来两个句子是否相邻的。对此也有人提出了改进方法SOP，SOP更加侧重于判断两个句子的前后关系，而不是两个句子是否相邻。</p><h4 id="Tips-BERT预训练是多任务模型的预训练，因此NSP和MLM两个任务的训练是同时进行的。"><a href="#Tips-BERT预训练是多任务模型的预训练，因此NSP和MLM两个任务的训练是同时进行的。" class="headerlink" title="Tips: BERT预训练是多任务模型的预训练，因此NSP和MLM两个任务的训练是同时进行的。"></a>Tips: BERT预训练是多任务模型的预训练，因此NSP和MLM两个任务的训练是同时进行的。</h4><h2 id="三-BERT微调fine-tune"><a href="#三-BERT微调fine-tune" class="headerlink" title="三. BERT微调fine-tune"></a>三. BERT微调fine-tune</h2><h3 id="1-自然语言推理-Natural-Language-Inference"><a href="#1-自然语言推理-Natural-Language-Inference" class="headerlink" title="1. 自然语言推理(Natural Language Inference)"></a>1. 自然语言推理(Natural Language Inference)</h3><p>自然语言推理研究是否有假设可以从前提推断出来，其中两者都是文本序列。换句话说，自然语言推理决定了一对文本序列之间的逻辑关系。这种关系通常分为三种类型：</p><ul><li><em>蕴含（entailment）</em>：假设可以从前提中推断出来。</li><li><em>矛盾（contradiction）</em>：可以从前提中推断假设的否定。</li><li><em>中立（neutral）</em>：所有其他情况。</li></ul><p>自然语言推理也被称为识别文本蕴含任务。例如，下面的词元对将被标记为<em>蕴含（entailment）</em>，因为假设中的 “显示亲情” 可以从前提中的 “相互拥抱” 推断出来。</p><blockquote><p>前提：两个女人互相拥抱。</p><p>假设：两个女人表现出亲情。</p></blockquote><p>以下是 <em>矛盾</em> 的例子，因为 “运行编码示例” 表示 “没有睡觉” 而不是 “睡觉”。</p><blockquote><p>前提：一个男人正在运行从 “潜入深度学习” 中的编码示例。</p><p>假设：那个男人在睡觉。</p></blockquote><p>第三个例子显示了 <em> 中立性 </em> 关系，因为 “正在为我们表演” 这一事实既不能推断 “著名的” 也不是 “不出名”。</p><blockquote><p>前提：音乐家们正在为我们表演。</p><p>假设：音乐家很有名。</p></blockquote><h3 id="2-NLI微调模型"><a href="#2-NLI微调模型" class="headerlink" title="2. NLI微调模型"></a>2. NLI微调模型</h3><p>​        BERT在预训练好之后就可以通过更改输出层来完成不同的任务，本节所讲的fine-tune应用是自然语言推理Natural Language Inference</p><p><img src="https://z3.ax1x.com/2021/11/16/IheluR.png" style="zoom: 80%;" /></p><p>本次微调所用的输出层很简单，直接在BERT输出层添加一个多层感知机。</p><h3 id="3-SNLI数据集"><a href="#3-SNLI数据集" class="headerlink" title="3. SNLI数据集"></a>3. SNLI数据集</h3><p>To Be Continued !</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>英文论文套话</title>
      <link href="/2021/11/14/%E8%8B%B1%E6%96%87%E8%AE%BA%E6%96%87%E5%A5%97%E8%AF%9D/"/>
      <url>/2021/11/14/%E8%8B%B1%E6%96%87%E8%AE%BA%E6%96%87%E5%A5%97%E8%AF%9D/</url>
      
        <content type="html"><![CDATA[<h1 id="一-常用句式"><a href="#一-常用句式" class="headerlink" title="一. 常用句式"></a>一. 常用句式</h1><h2 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1. Abstract"></a>1. Abstract</h2><h2 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h2><p>In recent years, DG has received increasing attention from the research community due to its importance to practical applications.</p><p>To overcome the domain shift problem, as well as the absence of target data, the problem of domain generalization (DG) is introduced .</p><p>In this survey paper, we aim to provide a timely and comprehensive literature review.   </p><p>In the context of DG,(就DG而言)</p><h2 id="3-Related-work"><a href="#3-Related-work" class="headerlink" title="3. Related work"></a>3. Related work</h2><h2 id="4-Formulation"><a href="#4-Formulation" class="headerlink" title="4. Formulation"></a>4. Formulation</h2><h2 id="5-Implementation"><a href="#5-Implementation" class="headerlink" title="5. Implementation"></a>5. Implementation</h2><h2 id="6-Results"><a href="#6-Results" class="headerlink" title="6. Results"></a>6. Results</h2><h2 id="7-Limitations-and-Discussion"><a href="#7-Limitations-and-Discussion" class="headerlink" title="7. Limitations and Discussion"></a>7. Limitations and Discussion</h2><h1 id="二-经典替换词"><a href="#二-经典替换词" class="headerlink" title="二. 经典替换词"></a>二. 经典替换词</h1>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>自注意力</title>
      <link href="/2021/10/14/self_attention/"/>
      <url>/2021/10/14/self_attention/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>我们前面说过，注意力机制包含几个重要的参数，query，key，value，针对不同的问题，往往需要选择合适的变量来作为query，key,和value，当遇到query,key,value都是同一个东西，同一种参数的时候，这样的机制叫做自注意力机制。</p><h2 id="1-自注意力"><a href="#1-自注意力" class="headerlink" title="1. 自注意力"></a>1. 自注意力</h2><p>假设我们有一个输入序列</p><script type="math/tex; mode=display">\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}, \forall \mathbf{x}_{i} \in \mathbb{R}^{d}</script><p>xi是第i个时间步的输入，d是输入值的特征维度，自注意力池化层将xi同时作为query,key,value，对序列抽取特征得到</p><script type="math/tex; mode=display">\mathbf{y}_{1}, \ldots, \mathbf{y}_{n}</script><p>其中  </p><script type="math/tex; mode=display">\mathbf{y}_{i}=f\left(\mathbf{x}_{i},\left(\mathbf{x}_{1}, \mathbf{x}_{1}\right), \ldots,\left(\mathbf{x}_{n}, \mathbf{x}_{n}\right)\right) \in \mathbb{R}^{d}</script><p>xi作为query，key-value对包含了所有的xi对，对应序列中的每一个元素xi都会输出一个yi。</p><p><a href="https://imgtu.com/i/f4TBZ9"><img src="https://z3.ax1x.com/2021/08/17/f4TBZ9.png" alt="f4TBZ9.png"></a></p><h2 id="2-与CNN-RNN对比"><a href="#2-与CNN-RNN对比" class="headerlink" title="2. 与CNN,RNN对比"></a>2. 与CNN,RNN对比</h2><p><a href="https://imgtu.com/i/f4q77Q"><img src="https://z3.ax1x.com/2021/08/17/f4q77Q.png" alt="f4q77Q.png"></a></p><p>对于CNN，k就是指卷积核的大小，n是输入数据量，d是指数据维度，并行度就是说每个输出之间可以各自独立运算出结果，第一个输出的结果不取决于上一个输出，这样的话就很方便进行并行的计算，大家可以同时计算，这样的计算效率就比较高。<br><a href="https://imgtu.com/i/f4OIoQ"><img src="https://z3.ax1x.com/2021/08/17/f4OIoQ.png" alt="f4OIoQ.png"></a></p><p>最长路径我理解的是输入信息在前向计算和反向传播过程中影响到另一个输入所需要走过的最长路径。</p><p><a href="https://imgtu.com/i/f4quYn"><img src="https://z3.ax1x.com/2021/08/17/f4quYn.png" alt="f4quYn.png"></a></p><p>自注意力的计算复杂度比较高，尤其是输入序列较长的时候，根据之前计算yi的公式，我们需要把序列中的每个元素都进行计算，这带来了比较大的计算负担。但是最大路径很短，这在网路示意图中可以很明显的看到，当序列很长的时候可以很快获取到距离比较远的信息。且相较于RNN多级传递可以有较少的信息损失。</p><p>我们都知道RNN适合处理序列但处理长序列就会有长程依赖的问题，为了解决这个问题提出了LSTM，但LSTM正如它的名字长短时记忆网络，终究只是比较长的短时记忆网络，原有局限性有改善但是依旧存在，因此结合自注意力池化可以做得更好，付出的代价就是高昂的计算成本。</p><h2 id="3-位置编码"><a href="#3-位置编码" class="headerlink" title="3. 位置编码"></a>3. 位置编码</h2><p>跟CNN和RNN不同，自注意力并没有记录位置信息。</p><p><a href="https://imgtu.com/i/f5pDXR"><img src="https://z3.ax1x.com/2021/08/17/f5pDXR.png" alt="f5pDXR.png" style="zoom:50%;" /></a></p><p>意思就是当我输入序列顺序打乱，输出的顺序也会打乱，但是对应位置上的输出本身内容并不会发生改变，但是我们知道在处理序列的时候元素的顺序也是信息的一部分，比如在在翻译任务里面语序的不同往往对应不同的输出。</p><p>一个解决办法就是把位置信息添加到输入序列中，让输入数据本身就带有位置信息。</p><h4 id="位置矩阵"><a href="#位置矩阵" class="headerlink" title="位置矩阵"></a>位置矩阵</h4><p>假设长度为n的序列是$\mathbf{X} \in \mathbb{R}^{n \times d}$，那么使用一个位置编码矩阵$\mathbf{P} \in \mathbb{R}^{n \times d}$,​将P加在X上，将X+P作为输入。其中P本身包含了许多关于X元素的位置信息，那么我们的目标就是找到一个合理的算法能够有效的提取X的位置信息并且存储在P里面。</p><p>P的计算公式如下：</p><p>$p_{i, 2 j}=\sin \left(\frac{i}{10000^{2 j / d}}\right), \quad p_{i, 2 j+1}=\cos \left(\frac{i}{10000^{2 j / d}}\right)$</p><p>i代表序列的第几个元素，即所谓的位置，j代表第几个特征维度。奇数列和偶数列不一样。</p><p>这是位置矩阵的图像。</p><p><a href="https://imgtu.com/i/f5PQpD"><img src="https://z3.ax1x.com/2021/08/17/f5PQpD.png" alt="f5PQpD.png"  /></a></p><p>途中的Row(position)对应的是位置矩阵的n，即第几个元素，不同的col列对应不同的特征维度。</p><p>模型需要有一定能力才能够学习到输入数据和位置信息之间的关系，这个位置矩阵存储的是相对位置信息。</p><p>我觉得可以借助信号处理的知识来理解这个位置矩阵，在信号与系统我们可以知道，对于任意一个时序信号我们都可以通过傅里叶变换将一个信号变换到频域，这个信号就会存在许多频域的分量。每一个频率分量在时域上进行叠加就得到一个完整的信号。</p><p>我们可以将输入序列X的每一维特征类比到频域的每一个频率分量即col6,col7都对应一个频率分量，然后不同的位置i或者时间点t即Row（position）就对应输入信号的不同的相位。不同的时间点i之间的差就可以类比相位差。</p><p>我们知道正弦信号具有周期性，所以其实相位差代表的就是相对位置关系，所以我们可以认为位置矩阵存储的是相对位置关系。</p><p><a href="https://imgtu.com/i/f5Abse"><img src="https://z3.ax1x.com/2021/08/17/f5Abse.png" alt="f5Abse.png" style="zoom: 80%;" /></a></p><h4 id="绝对位置信息"><a href="#绝对位置信息" class="headerlink" title="绝对位置信息"></a>绝对位置信息</h4><p>或者可以借助计算机使用的二进制编码来理解</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0 in binary is 000</span><br><span class="line">1 in binary is 001</span><br><span class="line">2 in binary is 010</span><br><span class="line">3 in binary is 011</span><br><span class="line">4 in binary is 100</span><br><span class="line">5 in binary is 101</span><br><span class="line">6 in binary is 110</span><br><span class="line">7 in binary is 111</span><br></pre></td></tr></table></figure><p>我们也可以将$0~7$这个序列的每个元素都由长度为3的特征表示，低位的特征变化频率就比高位的变化要快。</p><p><a href="https://imgtu.com/i/f5mh3q"><img src="https://z3.ax1x.com/2021/08/17/f5mh3q.png" alt="f5mh3q.png"></a></p><h4 id="相对位置信息"><a href="#相对位置信息" class="headerlink" title="相对位置信息"></a>相对位置信息</h4><p>位置位于$i+\delta$​处的位置编码可由线性投影位置$i$​处的位置编码来表示​</p><p>记$\omega_{j}=1 / 10000^{2 j / d}$，那么</p><p>$\left[\begin{array}{cc}\cos \left(\delta \omega_{j}\right) &amp; \sin \left(\delta \omega_{j}\right) \\ -\sin \left(\delta \omega_{j}\right) &amp; \cos \left(\delta \omega_{j}\right)\end{array}\right]\left[\begin{array}{c}p_{i, 2 j} \\ p_{i, 2 j+1}\end{array}\right]=\left[\begin{array}{c}p_{i+\delta, 2 j} \\ p_{i+\delta, 2 j+1}\end{array}\right]$</p><p>这样就意味着输入序列的两个元素之间的相对位置关系固定的情况下，不管其中一个元素的绝对位置在哪，它们之间的相对位置关系都可以用同一个投影矩阵表示，毕竟这个投影矩阵和绝对位置$i$​没有关系。如果要用一个参数$W$​，表示相对位置关系采用这样的形式就可以不用顾及元素出现在序列的哪一个位置。</p><p>其实也可以用相位差来类比，正弦信号相乘会引起相位变化。</p><p><a href="https://imgtu.com/i/f5Kkct"><img src="https://z3.ax1x.com/2021/08/17/f5Kkct.md.jpg" alt="f5Kkct.md.jpg"></a></p><p>可以看到，在与投影矩阵相乘之后，原来的正弦函数发生了相位偏移。</p><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><ul><li>自注意力池化层将$x_i$​当作key,value,query来对序列抽取特征​</li><li>完全并行，最长序列为1(对于任何一个输出都参考了整个序列的信息)，长序列计算成本高</li><li>位置编码在输入中加入位置信息，使得自注意力能够记忆位置信息。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/10/14/hello-world/"/>
      <url>/2021/10/14/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
